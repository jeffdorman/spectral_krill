{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bbb074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "from osgeo import gdal, gdal_array\n",
    "from datetime import datetime, time\n",
    "import matplotlib.path as mpltPath\n",
    "import xml.etree.cElementTree as et\n",
    "import pandas as pd\n",
    "import wv2_metadata as wv2\n",
    "import rpm_equation as rpm\n",
    "import pickle\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from netCDF4 import Dataset  \n",
    "import time\n",
    "\n",
    "#n1=['06','07','08','09','10','11','12']\n",
    "#n2=['01','02','03','04','05','06','07']\n",
    "\n",
    "#b1='/Volumes/Elements/AUS_MAXAR_Images_Request_68/'\n",
    "#b2='WV02-2012-11-21/WV2201211210035'\n",
    "#b3='M00/12NOV210035'\n",
    "#b4='-M1BS-507101033020_01_P0'\n",
    "#b5='/12NOV210035'\n",
    "#b6='.NTF'\n",
    "#b7='.XML'\n",
    "ntf_fname=[]\n",
    "xml_fname=[]\n",
    "for no in range (0,len(n1)):\n",
    "    ntf_fname.append(b1 + b2 + n1[no] + b3 + n1[no] + b4 + n2[no] + b5 + n1[no] + b4 + n2[no] + b6)\n",
    "    xml_fname.append(b1 + b2 + n1[no] + b3 + n1[no] + b4 + n2[no] + b5 + n1[no] + b4 + n2[no] + b7)\n",
    "\n",
    "data=gdal.Open(ntf_fname[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906464e8-ff6e-44d1-8d98-5ee84ce24512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wv2_filename_generator as wv2fg\n",
    "\n",
    "a, b, c = wv2fg.wv2_filename_generator('2010-04-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5639398a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/1030010005B12D00/WV02_20100418004950_1030010005B12D00_10APR18004950-M1BS-500081136190_01_P005.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/1030010005B12D00/WV02_20100418004951_1030010005B12D00_10APR18004951-M1BS-500081136190_01_P006.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/1030010005B12D00/WV02_20100418004954_1030010005B12D00_10APR18004954-M1BS-500081136190_01_P008.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/1030010005B12D00/WV02_20100418004955_1030010005B12D00_10APR18004955-M1BS-500081136190_01_P009.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/1030010005B12D00/WV02_20100418004956_1030010005B12D00_10APR18004956-M1BS-500081136190_01_P010.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/103001000520A400/WV02_20100418005019_103001000520A400_10APR18005019-M1BS-500081135160_01_P007.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/103001000520A400/WV02_20100418005020_103001000520A400_10APR18005020-M1BS-500081135160_01_P008.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/103001000520A400/WV02_20100418005022_103001000520A400_10APR18005022-M1BS-500081135160_01_P009.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/103001000520A400/WV02_20100418005023_103001000520A400_10APR18005023-M1BS-500081135160_01_P010.ntf',\n",
       " '/Volumes/Elements/AUS_MAXAR_Images_Request_68/WV02-2010-04-18/103001000520A400/WV02_20100418005023_103001000520A400_10APR18005023-M1BS-500081135160_01_P011.ntf']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import cluster\n",
    "from osgeo import gdal, gdal_array\n",
    "from datetime import datetime, time\n",
    "import matplotlib.path as mpltPath\n",
    "import xml.etree.cElementTree as et\n",
    "import pandas as pd\n",
    "import wv2_metadata as wv2\n",
    "import rpm_equation as rpm\n",
    "import pickle\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from netCDF4 import Dataset  \n",
    "import time\n",
    "import wv2_filename_generator as wv2fg\n",
    "\n",
    "#ntf_fname=['data/aust_wv2_20121121/12NOV21003506-M1BS-507101033020_01_P001.NTF', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003507-M1BS-507101033020_01_P002.NTF', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003508-M1BS-507101033020_01_P003.NTF', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003509-M1BS-507101033020_01_P004.NTF', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003510-M1BS-507101033020_01_P005.NTF', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003511-M1BS-507101033020_01_P006.NTF', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003512-M1BS-507101033020_01_P007.NTF']\n",
    "\n",
    "#xml_fname=['data/aust_wv2_20121121/12NOV21003506-M1BS-507101033020_01_P001.xml', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003507-M1BS-507101033020_01_P002.xml', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003508-M1BS-507101033020_01_P003.xml', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003509-M1BS-507101033020_01_P004.xml', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003510-M1BS-507101033020_01_P005.xml', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003511-M1BS-507101033020_01_P006.xml', \\\n",
    "#           'data/aust_wv2_20121121/12NOV21003512-M1BS-507101033020_01_P007.xml']\n",
    "#n1=['06','07','08','09','10','11','12']\n",
    "#n2=['01','02','03','04','05','06','07']\n",
    "\n",
    "#b1='/Volumes/Elements/AUS_MAXAR_Images_Request_68/'\n",
    "#b2='WV02-2012-11-21/WV2201211210035'\n",
    "#b3='M00/12NOV210035'\n",
    "#b4='-M1BS-507101033020_01_P0'\n",
    "#b5='/12NOV210035'\n",
    "#b6='.NTF'\n",
    "#b7='.XML'\n",
    "#ntf_fname=[]\n",
    "#xml_fname=[]\n",
    "\n",
    "ntf_fname, xml_fname, outtext = wv2fg.wv2_filename_generator('2010-04-18')\n",
    "\n",
    "\n",
    "\n",
    "#for no in range (0,len(n1)):\n",
    "#    ntf_fname.append(b1 + b2 + n1[no] + b3 + n1[no] + b4 + n2[no] + b5 + n1[no] + b4 + n2[no] + b6)\n",
    "#    xml_fname.append(b1 + b2 + n1[no] + b3 + n1[no] + b4 + n2[no] + b5 + n1[no] + b4 + n2[no] + b7)\n",
    "\n",
    "tic=time.time()\n",
    "for i in range(0,len(ntf_fname)):\n",
    "    with open(outtext, \"a\") as text_file: text_file.write(ntf_fname[i] + \"\\n\")\n",
    "    print(ntf_fname[i])\n",
    "    #Download Flag is reset each time a new file is opened...\\\n",
    "    #Helps ensure you don't download the data more than once as it is very time-consuming.\n",
    "    download_flag=0\n",
    "    \n",
    "    data=gdal.Open(ntf_fname[i])\n",
    "    ncols = data.RasterXSize\n",
    "    nrows = data.RasterYSize\n",
    "    #print('Ncols  = ', str(ncols), ' and Nrows = ', str(nrows))\n",
    "    \n",
    "    with open(outtext, \"a\") as text_file: text_file.write(xml_fname[i] + \"\\n\")\n",
    "    print(xml_fname[i])\n",
    "    tree=et.parse(xml_fname[i])\n",
    "    root=tree.getroot()\n",
    "    \n",
    "    UL_lon, UL_lat, UL_hae, UR_lon, UR_lat, UR_hae, \\\n",
    "    LL_lon, LL_lat, LL_hae, LR_lon, LR_lat, LR_hae = wv2.extract_latlon_data(root)\n",
    "    \n",
    "    Up_lat=((UL_lat + UR_lat)/2)\n",
    "    Lo_lat=((LL_lat + LR_lat)/2)\n",
    "    Le_lon=((UL_lon + LL_lon)/2)\n",
    "    Ri_lon=((UR_lon + LR_lon)/2)\n",
    "    \n",
    "    #This is an approximation as ncols and nrows will be updated later to remove nul columns and rows\n",
    "    #if and only if data overlaps with the polygons of interest.  Dont want to have to download the data\n",
    "    #unless I have to to avoid computaional time.  \n",
    "    jlon=np.linspace(Le_lon,Ri_lon,ncols)\n",
    "    jlonshort=jlon[0::100]\n",
    "    jlat=np.linspace(Up_lat,Lo_lat,nrows)\n",
    "    jlatshort=jlat[0::100]\n",
    "    \n",
    "    pg_fnames=['data/aust1.csv', 'data/aust2.csv', 'data/aust3.csv', 'data/aust4.csv', \\\n",
    "           'data/aust5.csv', 'data/aust6.csv', 'data/aust7.csv', 'data/aust8.csv', \\\n",
    "           'data/aust9.csv', 'data/aust10.csv', 'data/aust11.csv', 'data/aust1213.csv', \\\n",
    "           'data/aust14.csv', 'data/aust15.csv', 'data/aust16.csv', 'data/aust17.csv', \\\n",
    "           'data/aust18.csv', 'data/aust19.csv']\n",
    "    #pg_fnames=['data/aust15.csv']\n",
    "    for pg in range(0,len(pg_fnames)):\n",
    "\n",
    "        #Read in the polygon you are interested in getting data within.\n",
    "        df=pd.read_csv(pg_fnames[pg])\n",
    "        lon=df['POINT_X'].values.tolist()\n",
    "        lat=df['POINT_Y'].values.tolist()\n",
    "        #polygon_shape = list(zip(lon,lat))\n",
    "\n",
    "        polygon = Polygon([(lon[0], lat[0]), (lon[1], lat[1]), (lon[2], lat[2]), (lon[3], lat[3])])\n",
    "\n",
    "        shortmask=np.ones([len(jlatshort),len(jlonshort)])\n",
    "        for ii in range(0,len(jlatshort)):\n",
    "            for jj in range(0,len(jlonshort)):\n",
    "                shortmask[ii,jj]=polygon.contains(Point(jlonshort[jj],jlatshort[ii]))\n",
    "\n",
    "        S_mask=np.array(shortmask,dtype=bool)\n",
    "        if np.sum(S_mask)==0:\n",
    "            with open(outtext, \"a\") as text_file: text_file.write('No Data in '+ pg_fnames[pg] + \"\\n\")\n",
    "        else:\n",
    "            #Now Download the data and reset the lat/lon arrays only once.  Next polygon through it will \n",
    "            # have already been done and the above code doesn't need to be redone...\n",
    "            if download_flag==0:\n",
    "                b2=data.GetRasterBand(2)\n",
    "                d2=b2.ReadAsArray()\n",
    "                b3=data.GetRasterBand(3)\n",
    "                d3=b3.ReadAsArray()\n",
    "                b5=data.GetRasterBand(5)\n",
    "                d5=b5.ReadAsArray()\n",
    "\n",
    "                #Removing Rows that are entirely filled with zeros\n",
    "                loar=d2==0\n",
    "                rows=np.all(loar,axis=1)\n",
    "                rwidx=np.where(rows)\n",
    "                rwidx=rwidx[0][:]\n",
    "                if (rwidx[-1]-rwidx[0]+1)==len(rwidx):\n",
    "                    #delete the rows\n",
    "                    dd2=d2[0:rwidx[0]][:]\n",
    "                    dd3=d3[0:rwidx[0]][:]\n",
    "                    dd5=d5[0:rwidx[0]][:]\n",
    "\n",
    "                #Removing Columns that are entirely filled with zeros\n",
    "                loar=dd2==0\n",
    "                cols=np.all(loar,axis=0)\n",
    "                colidx=np.where(cols)\n",
    "                colidx=colidx[0][:]\n",
    "                if (colidx[-1]-colidx[0]+1)==len(colidx):\n",
    "                    #delete the columns\n",
    "                    ddd2=dd2[:,0:colidx[0]]\n",
    "                    ddd3=dd3[:,0:colidx[0]]\n",
    "                    ddd5=dd5[:,0:colidx[0]]\n",
    "\n",
    "                #Defining new lat lon arrays that correspond to the reduced numbers of rows and columns\n",
    "                nrows,ncols=np.shape(ddd2)\n",
    "                #print('Ncols  = ', str(ncols), ' and Nrows = ', str(nrows))\n",
    "                jlon=np.linspace(Le_lon,Ri_lon,ncols)\n",
    "                jlonshort=jlon[0::100]\n",
    "                jlat=np.linspace(Up_lat,Lo_lat,nrows)\n",
    "                jlatshort=jlat[0::100]\n",
    "                \n",
    "                #And Redefining the mask to extract the data\n",
    "                shortmask=np.ones([len(jlatshort),len(jlonshort)])\n",
    "                for ii in range(0,len(jlatshort)):\n",
    "                    for jj in range(0,len(jlonshort)):\n",
    "                        shortmask[ii,jj]=polygon.contains(Point(jlonshort[jj],jlatshort[ii]))\n",
    "\n",
    "                S_mask=np.array(shortmask,dtype=bool)\n",
    "                download_flag=1\n",
    "            \n",
    "            rwcol=np.column_stack(np.where(S_mask))\n",
    "            rwmin=np.min(rwcol[:,0])*100\n",
    "            rwmax=np.max(rwcol[:,0])\n",
    "            colmin=np.min(rwcol[:,1])*100\n",
    "            colmax=np.max(rwcol[:,1])\n",
    "\n",
    "            if rwmax+1==len(jlatshort):\n",
    "                rwmax=len(jlat)\n",
    "            else:\n",
    "                rwmax=rwmax*100\n",
    "            if colmax+1==len(jlonshort):\n",
    "                colmax=len(jlon)\n",
    "            else:\n",
    "                colmax=colmax*100\n",
    "\n",
    "            #print('Row Min = ' + str(rwmin) + ' and Row Max = ' + str(rwmax))\n",
    "            #print('Col Min = ' + str(colmin) + ' and Col Max = ' + str(colmax))\n",
    "\n",
    "            L_mask=np.zeros(np.shape(ddd2))\n",
    "            L_mask[rwmin:rwmax,colmin:colmax]=1\n",
    "            with open(outtext, \"a\") as text_file: text_file.write('There are ' + str(np.sum(L_mask)) + ' records in ' +  pg_fnames[pg] + \"\\n\")\n",
    "            L_mask=np.array(L_mask,dtype=bool)\n",
    "\n",
    "            final_d2=ddd2[L_mask]\n",
    "            final_d2=np.reshape(final_d2,(rwmax-rwmin,colmax-colmin))\n",
    "            final_d3=ddd3[L_mask]\n",
    "            final_d3=np.reshape(final_d3,(rwmax-rwmin,colmax-colmin))\n",
    "            final_d5=ddd5[L_mask]\n",
    "            final_d5=np.reshape(final_d5,(rwmax-rwmin,colmax-colmin))\n",
    "\n",
    "            final_lon=jlon[colmin:colmax]\n",
    "            final_lat=jlat[rwmin:rwmax]\n",
    "\n",
    "            try: ncfile.close()  # just to be safe, make sure dataset is not already open.\n",
    "            except: pass\n",
    "            nc_fname=ntf_fname[i][0:-4] + '_' + pg_fnames[pg][5:-4] + '.nc'\n",
    "            ncfile = Dataset(nc_fname,mode='w',format='NETCDF4_CLASSIC') \n",
    "            #print(ncfile)\n",
    "            lat_dim = ncfile.createDimension('lat', len(final_lat))     # latitude axis\n",
    "            lon_dim = ncfile.createDimension('lon', len(final_lon))    # longitude axis\n",
    "            time_dim = ncfile.createDimension('time', None) # unlimited axis (can be appended to).\n",
    "            #for dim in ncfile.dimensions.items():\n",
    "            #    print(dim)\n",
    "            lat = ncfile.createVariable('lat', np.float32, ('lat',))\n",
    "            lat.units = 'degrees_north'\n",
    "            lat.long_name = 'latitude'\n",
    "            lon = ncfile.createVariable('lon', np.float32, ('lon',))\n",
    "            lon.units = 'degrees_east'\n",
    "            lon.long_name = 'longitude'\n",
    "            #time = ncfile.createVariable('time', np.float64, ('time',))\n",
    "            #time.units = 'hours since 1800-01-01'\n",
    "            #time.long_name = 'time'\n",
    "            # Define a 3D variable to hold the data\n",
    "            #temp = ncfile.createVariable('temp',np.float64,('time','lat','lon')) # note: unlimited dimension is leftmost\n",
    "            band2 = ncfile.createVariable('band2',np.float64,('lat','lon')) # note: unlimited dimension is leftmost\n",
    "            band2.units = 'DigitalNumber' # degrees Kelvin\n",
    "            band2.standard_name = 'digital_number' # this is a CF standard name\n",
    "            band3 = ncfile.createVariable('band3',np.float64,('lat','lon')) # note: unlimited dimension is leftmost\n",
    "            band3.units = 'DigitalNumber' # degrees Kelvin\n",
    "            band3.standard_name = 'digital_number' # this is a CF standard name\n",
    "            band5 = ncfile.createVariable('band5',np.float64,('lat','lon')) # note: unlimited dimension is leftmost\n",
    "            band5.units = 'DigitalNumber' # degrees Kelvin\n",
    "            band5.standard_name = 'digital_number' # this is a CF standard name\n",
    "            #print(band2)\n",
    "\n",
    "            lat[:]=final_lat\n",
    "            lon[:]=final_lon\n",
    "            band2[:,:]=final_d2\n",
    "            band3[:,:]=final_d3\n",
    "            band5[:,:]=final_d5\n",
    "\n",
    "            # first print the Dataset object to see what we've got\n",
    "            #print(ncfile)\n",
    "            # close the Dataset.\n",
    "            ncfile.close(); print(nc_fname + ' is closed!')\n",
    "            with open(outtext, \"a\") as text_file: text_file.write(nc_fname + ' is closed!' + \"\\n\")\n",
    "            print('')\n",
    "\n",
    "toc=time.time()\n",
    "with open(outtext, \"a\") as text_file: text_file.write('Elapsed Time is ' + str(toc-tic) + \"\\n\")\n",
    "print('Elapsed Time is ' + str(toc-tic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246f116",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8125653",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3a4fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "    b2=data.GetRasterBand(2)\n",
    "    d2=b2.ReadAsArray()\n",
    "    b3=data.GetRasterBand(3)\n",
    "    d3=b3.ReadAsArray()\n",
    "    b5=data.GetRasterBand(5)\n",
    "    d5=b5.ReadAsArray()\n",
    "    \n",
    "    loar=d2==0\n",
    "\n",
    "    rows=np.all(loar,axis=1)\n",
    "    rwidx=np.where(rows)\n",
    "    rwidx=rwidx[0][:]\n",
    "    if (rwidx[-1]-rwidx[0]+1)==len(rwidx):\n",
    "        #delete the rows\n",
    "        dd2=d2[0:rwidx[0]][:]\n",
    "        dd3=d3[0:rwidx[0]][:]\n",
    "        dd5=d5[0:rwidx[0]][:]\n",
    "\n",
    "    loar=dd2==0\n",
    "    cols=np.all(loar,axis=0)\n",
    "    colidx=np.where(cols)\n",
    "    colidx=colidx[0][:]\n",
    "    if (colidx[-1]-colidx[0]+1)==len(colidx):\n",
    "        #delete the columns\n",
    "        ddd2=dd2[:,0:colidx[0]]\n",
    "        ddd3=dd3[:,0:colidx[0]]\n",
    "        ddd5=dd5[:,0:colidx[0]]\n",
    "        \n",
    "\n",
    "    \n",
    "    nrows,ncols=np.shape(ddd2)\n",
    "    print('Ncols  = ', str(ncols), ' and Nrows = ', str(nrows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc982911",
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_fnames=['data/aust1.csv', 'data/aust2.csv', 'data/aust3.csv', 'data/aust4.csv', \\\n",
    "           'data/aust5.csv', 'data/aust6.csv', 'data/aust7.csv', 'data/aust8.csv', \\\n",
    "           'data/aust9.csv', 'data/aust10.csv', 'data/aust11.csv', 'data/aust1213.csv', \\\n",
    "           'data/aust14.csv', 'data/aust15.csv', 'data/aust16.csv', 'data/aust17.csv', \\\n",
    "           'data/aust18.csv', 'data/aust19.csv']\n",
    "#pg_fnames=['data/aust15.csv']\n",
    "for pg in range(0,len(pg_fnames)):\n",
    "\n",
    "    #Read in the polygon you are interested in getting data within.\n",
    "    df=pd.read_csv(pg_fnames[pg])\n",
    "    lon=df['POINT_X'].values.tolist()\n",
    "    lat=df['POINT_Y'].values.tolist()\n",
    "    #polygon_shape = list(zip(lon,lat))\n",
    "\n",
    "    polygon = Polygon([(lon[0], lat[0]), (lon[1], lat[1]), (lon[2], lat[2]), (lon[3], lat[3])])\n",
    "\n",
    "    shortmask=np.ones([len(jlatshort),len(jlonshort)])\n",
    "    for i in range(0,len(jlatshort)):\n",
    "        for j in range(0,len(jlonshort)):\n",
    "            shortmask[i,j]=polygon.contains(Point(jlonshort[j],jlatshort[i]))\n",
    "    \n",
    "    S_mask=np.array(shortmask,dtype=bool)\n",
    "    if np.sum(S_mask)==0:\n",
    "        print('No Data in '+ pg_fnames[pg])\n",
    "    else:\n",
    "        rwcol=np.column_stack(np.where(S_mask))\n",
    "        rwmin=np.min(rwcol[:,0])*100\n",
    "        rwmax=np.max(rwcol[:,0])\n",
    "        colmin=np.min(rwcol[:,1])*100\n",
    "        colmax=np.max(rwcol[:,1])\n",
    "\n",
    "        if rwmax+1==len(jlatshort):\n",
    "            rwmax=len(jlat)\n",
    "        else:\n",
    "            rwmax=rwmax*100\n",
    "        if colmax+1==len(jlonshort):\n",
    "            colmax=len(jlon)\n",
    "        else:\n",
    "            colmax=colmax*100\n",
    "        \n",
    "        #print('Row Min = ' + str(rwmin) + ' and Row Max = ' + str(rwmax))\n",
    "        #print('Col Min = ' + str(colmin) + ' and Col Max = ' + str(colmax))\n",
    "        \n",
    "        L_mask=np.zeros(np.shape(ddd2))\n",
    "        L_mask[rwmin:rwmax,colmin:colmax]=1\n",
    "        print('There are ' + str(np.sum(L_mask)) + ' records in ' +  pg_fnames[pg])\n",
    "        L_mask=np.array(L_mask,dtype=bool)\n",
    "        \n",
    "        final_d2=ddd2[L_mask]\n",
    "        final_d2=np.reshape(final_d2,(rwmax-rwmin,colmax-colmin))\n",
    "        final_d3=ddd3[L_mask]\n",
    "        final_d3=np.reshape(final_d3,(rwmax-rwmin,colmax-colmin))\n",
    "        final_d5=ddd5[L_mask]\n",
    "        final_d5=np.reshape(final_d5,(rwmax-rwmin,colmax-colmin))\n",
    "        \n",
    "        final_lon=jlon[colmin:colmax]\n",
    "        final_lat=jlat[rwmin:rwmax]\n",
    "        \n",
    "        try: ncfile.close()  # just to be safe, make sure dataset is not already open.\n",
    "        except: pass\n",
    "        nc_fname=ntf_fname[0][0:-4] + '_' + pg_fnames[pg][5:-4] + '.nc'\n",
    "        ncfile = Dataset(nc_fname,mode='w',format='NETCDF4_CLASSIC') \n",
    "        #print(ncfile)\n",
    "        lat_dim = ncfile.createDimension('lat', len(final_lat))     # latitude axis\n",
    "        lon_dim = ncfile.createDimension('lon', len(final_lon))    # longitude axis\n",
    "        time_dim = ncfile.createDimension('time', None) # unlimited axis (can be appended to).\n",
    "        #for dim in ncfile.dimensions.items():\n",
    "        #    print(dim)\n",
    "        lat = ncfile.createVariable('lat', np.float32, ('lat',))\n",
    "        lat.units = 'degrees_north'\n",
    "        lat.long_name = 'latitude'\n",
    "        lon = ncfile.createVariable('lon', np.float32, ('lon',))\n",
    "        lon.units = 'degrees_east'\n",
    "        lon.long_name = 'longitude'\n",
    "        #time = ncfile.createVariable('time', np.float64, ('time',))\n",
    "        #time.units = 'hours since 1800-01-01'\n",
    "        #time.long_name = 'time'\n",
    "        # Define a 3D variable to hold the data\n",
    "        #temp = ncfile.createVariable('temp',np.float64,('time','lat','lon')) # note: unlimited dimension is leftmost\n",
    "        band2 = ncfile.createVariable('band2',np.float64,('lat','lon')) # note: unlimited dimension is leftmost\n",
    "        band2.units = 'DigitalNumber' # degrees Kelvin\n",
    "        band2.standard_name = 'digital_number' # this is a CF standard name\n",
    "        band3 = ncfile.createVariable('band3',np.float64,('lat','lon')) # note: unlimited dimension is leftmost\n",
    "        band3.units = 'DigitalNumber' # degrees Kelvin\n",
    "        band3.standard_name = 'digital_number' # this is a CF standard name\n",
    "        band5 = ncfile.createVariable('band5',np.float64,('lat','lon')) # note: unlimited dimension is leftmost\n",
    "        band5.units = 'DigitalNumber' # degrees Kelvin\n",
    "        band5.standard_name = 'digital_number' # this is a CF standard name\n",
    "        #print(band2)\n",
    "        \n",
    "        lat[:]=final_lat\n",
    "        lon[:]=final_lon\n",
    "        band2[:,:]=final_d2\n",
    "        band3[:,:]=final_d3\n",
    "        band5[:,:]=final_d5\n",
    "        \n",
    "        # first print the Dataset object to see what we've got\n",
    "        print(ncfile)\n",
    "        # close the Dataset.\n",
    "        ncfile.close(); print('Dataset is closed!')\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(xml_fname[i])\n",
    "    tree=et.parse(xml_fname[i])\n",
    "    root=tree.getroot()\n",
    "    \n",
    "    #Get Rigorous Projection Model Data from the .xml File\n",
    "    lat_off, lon_off, lat_sc, lon_sc, h_off, h_sc, \\\n",
    "    line_off, line_sc, samp_off, samp_sc, \\\n",
    "    lnc_arr, ldc_arr, snc_arr, sdc_arr = wv2.extract_RPC_data(root)\n",
    "    \n",
    "    #Get Satellite Parameters Data from the .xml File\n",
    "    b1_acf, b2_acf, b3_acf, b4_acf, b5_acf, b6_acf, b7_acf, b8_acf, \\\n",
    "    b1_ebw, b2_ebw, b3_ebw, b4_ebw, b5_ebw, b6_ebw, b7_ebw, b8_ebw, \\\n",
    "    MSE, dES = wv2.extract_conversion_params(root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
